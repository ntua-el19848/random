# Perceptron Problem

ÎŸ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎºÏÎ´Î¹ÎºÎ±Ï‚ Ï…Î»Î¿Ï€Î¿Î¹ÎµÎ¯ Î­Î½Î± Î±Ï€Î»ÏŒ Perceptron, Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Î¿ Ï€Î¹Î¿ Î²Î±ÏƒÎ¹ÎºÏŒÏ‚ Ï„ÏÏ€Î¿Ï‚ Î½ÎµÏ…ÏÏ‰Î½Î¹ÎºÎ¿Ï Î´Î¹ÎºÏ„ÏÎ¿Ï…, Î³Î¹Î± Î´Ï…Î±Î´Î¹ÎºÎ® Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ·. Î Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎµÎ¯Î½Î±Î¹ Î¼Î¯Î± Ï€ÎµÏÎ¹Î³ÏÎ±Ï†Î® Ï„Î¿Ï… Perceptron Ï„Î·Ï‚ Î¬ÏƒÎºÎ·ÏƒÎ·Ï‚:

1. **Î’Î±ÏƒÎ¹ÎºÎ® Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î±:**

    - ÎŸ Perceptron Î´Î­Ï‡ÎµÏ„Î±Î¹ Î´ÏÎ¿ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ ÎµÎ¹ÏƒÏŒÎ´Î¿Ï… (X1, X2)
    - Î ÏÎ¿ÏƒÏ€Î±Î¸ÎµÎ¯ Î½Î± Ï„Î±Î¾Î¹Î½Î¿Î¼Î®ÏƒÎµÎ¹ Ï„Î± ÏƒÎ·Î¼ÎµÎ¯Î± ÏƒÎµ Î´ÏÎ¿ ÎºÎ»Î¬ÏƒÎµÎ¹Ï‚ (+1 Î® -1)
    - Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î²Î¬ÏÎ· (w1, w2) ÎºÎ±Î¹ Î¼Î¹Î± Î¼ÎµÏÎ¿Î»Î·ÏˆÎ¯Î± (bias) Î³Î¹Î± Î½Î± ÎºÎ¬Î½ÎµÎ¹ Ï„Î·Î½ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·

2. **Î”Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚:**

    - Î‘ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î± Î²Î¬ÏÎ· Î¼Îµ Ï„Ï…Ï‡Î±Î¯ÎµÏ‚ Î¼Î¹ÎºÏÎ­Ï‚ Ï„Î¹Î¼Î­Ï‚
    - Î“Î¹Î± ÎºÎ¬Î¸Îµ ÏƒÎ·Î¼ÎµÎ¯Î¿ ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚:

        - Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ Ï„Î·Î½ Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·: sign(w1x1 + w2x2 + bias)
        - Î‘Î½ Î· Ï€ÏÏŒÎ²Î»ÎµÏˆÎ· ÎµÎ¯Î½Î±Î¹ Î»Î±Î½Î¸Î±ÏƒÎ¼Î­Î½Î·, ÎµÎ½Î·Î¼ÎµÏÏÎ½ÎµÎ¹ Ï„Î± Î²Î¬ÏÎ· ÎºÎ±Î¹ Ï„Î¿ bias


    - Î•Ï€Î±Î½Î±Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ Î¼Î­Ï‡ÏÎ¹ Î½Î± Î¼Î·Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î»Î¬Î¸Î· Î® Î½Î± Ï†Ï„Î¬ÏƒÎµÎ¹ Ï„Î¿Î½ Î¼Î­Î³Î¹ÏƒÏ„Î¿ Î±ÏÎ¹Î¸Î¼ÏŒ ÎµÏ€Î±Î½Î±Î»Î®ÏˆÎµÏ‰Î½


3. **Î”Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½:**

    - Î”Î¹Î±Î²Î¬Î¶ÎµÎ¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€ÏŒ Î±ÏÏ‡ÎµÎ¯Î± CSV
    - Î‘Î³Î½Î¿ÎµÎ¯ Ï„Î·Î½ Ï€ÏÏÏ„Î· Î³ÏÎ±Î¼Î¼Î® Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Ï„Î¹Ï‚ ÎµÏ€Î¹ÎºÎµÏ†Î±Î»Î¯Î´ÎµÏ‚
    - Î§Ï‰ÏÎ¯Î¶ÎµÎ¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÏƒÎµ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ (X1, X2) ÎºÎ±Î¹ ÎµÏ„Î¹ÎºÎ­Ï„ÎµÏ‚ (Label)


4. **Î‘Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ·:**

    - Î¥Ï€Î¿Î»Î¿Î³Î¯Î¶ÎµÎ¹ Ï„Î·Î½ Î±ÎºÏÎ¯Î²ÎµÎ¹Î± ÏƒÏ„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚
    - Î•Î»Î­Î³Ï‡ÎµÎ¹ Ï„Î·Î½ Î±Ï€ÏŒÎ´Î¿ÏƒÎ· ÏƒÎµ Î½Î­Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± (test data)
    - Î•Î¼Ï†Î±Î½Î¯Î¶ÎµÎ¹ Ï„Î± Ï„ÎµÎ»Î¹ÎºÎ¬ Î²Î¬ÏÎ· ÎºÎ±Î¹ Ï„Î¿ bias Ï„Î¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï…



Î— Î»ÏÏƒÎ· ÎµÎ¯Î½Î±Î¹ Ï…Î»Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î· ÏƒÎµ Python Ï‡Ï‰ÏÎ¯Ï‚ Ï„Î· Ï‡ÏÎ®ÏƒÎ· ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Ï‰Î½ Î²Î¹Î²Î»Î¹Î¿Î¸Î·ÎºÏÎ½ Î¼Î·Ï‡Î±Î½Î¹ÎºÎ®Ï‚ Î¼Î¬Î¸Î·ÏƒÎ·Ï‚ (ÎµÎºÏ„ÏŒÏ‚ Î±Ï€ÏŒ Ï„Î¿ numpy Î³Î¹Î± Î±ÏÎ¹Î¸Î¼Î·Ï„Î¹ÎºÎ¿ÏÏ‚ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î¿ÏÏ‚).


--- 

## The code

```python
import numpy as np  # numpy is used for efficient numerical operations

class Perceptron:

    # A simple implementation of the Perceptron algorithm for binary classification.
    # The perceptron learns to separate two classes by adjusting weights and bias.
    
    def __init__(self, learning_rate=0.1, max_iterations=1000):
        # Initialize the perceptron with learning parameters.
        # Args:
        #     learning_rate (float): How much to adjust weights on each error (between 0 and 1)
        #     max_iterations (int): Maximum number of times to iterate through training data
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None  # Will store the feature weights (w1, w2)
        self.bias = None     # Will store the bias term (b)
        
    def initialize_weights(self, n_features):
        # Initialize weights and bias to small random values.
        # Small random values are better than zeros as starting points.
        # Args:
        #     n_features (int): Number of input features (2 in this case: X1, X2)

        # Initialize weights with small random values (multiplied by 0.01 to keep them small)
        self.weights = np.random.randn(n_features) * 0.01
        self.bias = np.random.randn() * 0.01
    
    def predict_single(self, x):
        # Predict class for a single input using the perceptron formula:
        # f(x) = sign(wÂ·x + b)
        # where w is the weight vector, x is the input vector, and b is the bias
        # Args:
        #     x (numpy.array): Input feature vector [X1, X2]
        # Returns:
        #     int: 1 if activation >= 0, -1 otherwise

        # Calculate the dot product of weights and input features, then add bias
        activation = np.dot(x, self.weights) + self.bias
        # Apply the sign function: return 1 if activation >= 0, else -1
        return 1 if activation >= 0 else -1
    
    def predict(self, X):
        # Predict classes for multiple inputs.        
        # Args:
        #     X (numpy.array): Matrix of input features, shape (n_samples, n_features)
        # Returns:
        #     numpy.array: Array of predictions (1 or -1 for each input)
        return np.array([self.predict_single(x) for x in X])
    
    def train(self, X, y):
        # Train the perceptron using the perceptron learning algorithm.
        
        # The algorithm:
        # 1. For each training example:
        #     - Make a prediction
        #     - If prediction is wrong:
        #         * Update weights: w = w + learning_rate * y * x
        #         * Update bias: b = b + learning_rate * y
        # 2. Repeat until no errors or max iterations reached
        # Args:
        #     X (numpy.array): Training features, shape (n_samples, n_features)
        #     y (numpy.array): Training labels (1 or -1 for each sample)

        # Initialize weights if not already done
        if self.weights is None:
            self.initialize_weights(X.shape[1])
            
        # Training loop
        for iteration in range(self.max_iterations):
            misclassified = 0  # Counter for misclassified points in this iteration
            
            # Iterate through all training examples
            for xi, yi in zip(X, y):
                prediction = self.predict_single(xi)
                
                # Update weights and bias if prediction is wrong
                if prediction != yi:
                    # Calculate the update value
                    update = self.learning_rate * yi
                    # Update each weight: w_new = w_old + learning_rate * y * x
                    self.weights += update * xi
                    # Update bias: b_new = b_old + learning_rate * y
                    self.bias += update
                    misclassified += 1
            
            # If no misclassifications, we've found a solution
            if misclassified == 0:
                print(f"Converged after {iteration + 1} iterations")
                break
                
    def accuracy(self, X, y):
        # Calculate classification accuracy (percentage of correct predictions).
        # Args:
        #     X (numpy.array): Input features
        #     y (numpy.array): True labels            
        # Returns:
        #     float: Accuracy score between 0 and 1
        predictions = self.predict(X)
        return np.mean(predictions == y)  # Calculate percentage of correct predictions

def load_data(filename):
    # Load data from a CSV file and separate features and labels.
    # Expected CSV format:
    # X1,X2,Label
    # where X1 and X2 are real numbers and Label is either 1 or -1
    # Args:
    #     filename (str): Path to the CSV file 
    # Returns:
    #     tuple: (X, y) where X is features array and y is labels array

    data = np.loadtxt(filename, delimiter=',', skiprows=1)  # Load CSV file
    X = data[:, :2] # First two columns (X1, X2) are features
    y = data[:, 2]   # Last column is the label (1 or -1)
    return X, y

# Main execution block
if __name__ == "__main__":
    # Step 1: Load training data
    print("â³ Loading training data...")
    X_train, y_train = load_data('training_data.csv')
    
    # Step 2: Create and train perceptron
    print("ğŸ“ Creating and training perceptron...")
    perceptron = Perceptron(learning_rate=0.1, max_iterations=1000)
    perceptron.train(X_train, y_train)
    
    # Step 3: Calculate and display training accuracy
    train_accuracy = perceptron.accuracy(X_train, y_train)
    print(f"\nğŸ¯ Training accuracy: {train_accuracy:.2%}")
    
    # Step 4: Evaluate on test data
    print("\nğŸ” Evaluating on test data...")
    X_test, y_test = load_data('test_data.csv')
    test_accuracy = perceptron.accuracy(X_test, y_test)
    print(f"Test accuracy: {test_accuracy:.2%}")
    
    # Step 5: Display final model parameters
    print("\nFinal model parameters:")
    print(f"Weights (w1, w2): {perceptron.weights}")
    print(f"Bias (b): {perceptron.bias}")
```
---

## ÎŸÎ¹ Î­Î¾Î¿Î´Î¿Î¹ Ï„Î¿Ï… Ï€ÏÎ¿Î³ÏÎ¬Î¼Î¼Î±Ï„Î¿Ï‚
Î Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Ï†Î±Î¯Î½Î¿Î½Ï„Î±Î¹ screenshots Î¼Îµ Î´Î¹Î±Ï†ÏŒÏÎµÏ‚ ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÎ¹Ï‚ Ï„Î¿Ï… Ï€Î±ÏÎ±Ï€Î¬Î½Ï‰ ÎºÏÎ´Î¹ÎºÎ±. Î Î±ÏÎ±Ï„Î·ÏÎ¿ÏÎ¼Îµ Ï€Ï‰Ï‚ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î´Î¹Î±Ï†Î¿ÏÎ¬ Î±Ï€ÏŒ ÎµÎºÏ„Î­Î»ÎµÏƒÎ· ÏƒÎµ ÎµÎºÏ„Î­Î»ÎµÏƒÎ·, Ï„ÏŒÏƒÎ¿ ÏƒÏ„Î± Ï„ÎµÎ»Î¹ÎºÎ¬ weights ÏŒÏƒÎ¿ ÎºÎ±Î¹ ÏƒÏ„Î¿ Ï„ÎµÎ»Î¹ÎºÏŒ bias. Î‘Ï…Ï„ÏŒ ÏƒÏ…Î¼Î²Î±Î¯Î½ÎµÎ¹ ÎµÏ€ÎµÎ¹Î´Î® Î· Î±ÏÏ‡Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Ï‰Î½ Î±ÏÏ‡Î¹ÎºÏÎ½ Î²Î±ÏÏÎ½ Î³Î¯Î½ÎµÏ„Î±Î¹ Î¼Îµ Ï„Ï…Ï‡Î±Î¯ÎµÏ‚ Ï„Î¹Î¼Î­Ï‚.

![Screenshot 2025-01-02 225113](https://github.com/user-attachments/assets/f39c5fda-58ae-4c66-b214-89492716b40c)
![Screenshot 2025-01-02 225427](https://github.com/user-attachments/assets/21c88d6a-f044-43dd-b907-3a3159293c8a)
![Screenshot 2025-01-02 225454](https://github.com/user-attachments/assets/f05bb97d-f1cc-43ba-a895-b762b2c1449c)


